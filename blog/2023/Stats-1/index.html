<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Advanced Statistics (Part 1) | Suraj Bhor </title> <meta name="author" content="Suraj Bhor"> <meta name="description" content="Basic concepts of multivariate statistics"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://suraj-bhor.github.io/blog/2023/Stats-1/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Advanced Statistics (Part 1)",
            "description": "Basic concepts of multivariate statistics",
            "published": "May 05, 2023",
            "authors": [
              
              {
                "author": "Suraj Bhor",
                "authorURL": "https://suraj-bhor.github.io/",
                "affiliations": [
                  {
                    "name": "University of Tübingen, Germany",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Suraj Bhor </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Suraj Bhor </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Advanced Statistics (Part 1)</h1> <p>Basic concepts of multivariate statistics</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#data-matrix">Data Matrix</a> </div> <div> <a href="#covariance-matrix">Covariance Matrix</a> </div> <ul> <li> <a href="#mahalanobis-distance">Mahalanobis Distance</a> </li> </ul> <div> <a href="#probability-theory">Probability Theory</a> </div> <div> <a href="#multivariate-normal-distribution">Multivariate Normal Distribution</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/adv_stats_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="data-matrix">Data Matrix</h2> <p>As a start to the descriptive statistics, we will use \(n\) observations of \(p\) variables. The data matrix \(X\) is a \(n \times p\) matrix with \(n\) rows and \(p\) columns. Each row represents an observation and each column represents a variable. The \(i\)-th row of \(X\) is denoted by \(x_i^T\), where \(x_i\) is a \(p \times 1\) vector. The \(j\)-th column of \(X\) is denoted by \(x_j\), where \(x_j\) is a \(n \times 1\) vector. The \(i\)-th element of \(x_j\) is denoted by \(x_{ij}\).</p> <p>\(X\) = \(\begin{bmatrix} x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p} \\ x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np} \end{bmatrix}\)</p> <h3 id="mean-vector">Mean Vector</h3> <p>The mean vector \(\mu\) is a \(p \times 1\) vector with \(j\)-th element \(\mu_j\), which is the mean of the \(j\)-th variable. The \(j\)-th element of the mean vector is given by: \(\mu_j = \frac{1}{n} \sum_{i=1}^{n} x_{ij}\).</p> <p>If we stack up all these means, we get the mean vector: \(\overline{\mathbf{x}}=\frac{1}{n}\left(\begin{array}{c} \sum_{i=1}^n x_{i 1} \\ \sum_{i=1}^n x_{i 2} \\ \vdots \\ \sum_{i=1}^n x_{i p} \end{array}\right)=\left(\begin{array}{c} \bar{x}_1 \\ \bar{x}_2 \\ \vdots \\ \bar{x}_p \end{array}\right)\)</p> <h2 id="covariance-matrix">Covariance Matrix</h2> <p>The covariance matrix \(S\) is a \(p \times p\) matrix with \(j\)-th row and \(k\)-th column element \(S{jk}\), which is the covariance between the \(j\)-th and \(k\)-th variables. The \(j\)-th row and \(k\)-th column element of the covariance matrix is given by:</p> \[S_{jk} = \frac{1}{n-1} \sum_{i=1}^{n} (x_{ij} - \mu_j)(x_{ik} - \mu_k)\] <p>If we stack up all these covariances, we get the covariance matrix:</p> \[S=\frac{1}{n-1}\left(\begin{array}{cccc}\sum_{i=1}^n\left(x_{i 1}-\bar{x}_{1}\right)^{2} &amp; \sum_{i=1}^n\left(x_{i 1}-\bar{x}_{1}\right)\left(x_{i 2}-\bar{x}_{2}\right) &amp; \cdots &amp; \sum_{i=1}^n\left(x_{i 1}-\bar{x}_{1}\right)\left(x_{i p}-\bar{x}_{p}\right) \\ \sum_{i=1}^n\left(x_{i 2}-\bar{x}_{2}\right)\left(x_{i 1}-\bar{x}_{1}\right) &amp; \sum_{i=1}^n\left(x_{i 2}-\bar{x}_{2}\right)^{2} &amp; \cdots &amp; \sum_{i=1}^n\left(x_{i 2}-\bar{x}_{2}\right)\left(x_{i p}-\bar{x}_{p}\right) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \sum_{i=1}^n\left(x_{i p}-\bar{x}_{p}\right)\left(x_{i 1}-\bar{x}_{1}\right) &amp; \sum_{i=1}^n\left(x_{i p}-\bar{x}_{p}\right)\left(x_{i 2}-\bar{x}_{2}\right) &amp; \cdots &amp; \sum_{i=1}^n\left(x_{i p}-\bar{x}_{p}\right)^{2} \end{array}\right)\] <blockquote> <h4 id="question">Question</h4> <p>Why do we divide by \(n-1\) instead of \(n\) in the covariance matrix calculation? In the covariance matrix calculation, we divide by n-1 instead of n due to Bessel’s correction. This adjustment provides an unbiased estimate of the population covariance when working with sample data. Dividing by n-1 compensates for the underestimation caused by using the sample mean, which has one less degree of freedom than the population mean.</p> </blockquote> <p>The sample covariance matrix is then given by:</p> \[S=\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)\left(x_{i}-\overline{x}\right)^{T}\] <p>A part of this formula, \(x - \overline{x}\) can be used to calculate the Mahalanobis distance.</p> <h3 id="mahalanobis-distance">Mahalanobis Distance</h3> <p>The Mahalanobis distance is a measure of the distance between a point \(x\) and a distribution \(D\), introduced by P. C. Mahalanobis in 1936. It is a multi-dimensional generalization of the idea of measuring how many standard deviations away \(x\) is from the mean of \(D\). This distance is zero if \(x\) is at the mean of \(D\), and grows as \(x\) moves away from the mean along each principal component axis. The Mahalanobis distance is thus unitless and scale-invariant, and takes into account the correlations of the data set.</p> <p>The Mahalanobis distance between two vectors \(x\) and \(y\) with covariance matrix \(S\) is defined as:</p> \[d(x, y)=\sqrt{(x-y)^{T} S^{-1}(x-y)}\] <p>And the mahanalobis distance between a vector \(x\) and its mean vector \(\overline{x}\) with covariance matrix \(S\) is defined as:</p> \[d(x, \overline{x})=\sqrt{(x-\overline{x})^{T} S^{-1}(x-\overline{x})}\] <p>Geometrically speaking, these points build and ellipsoid around the mean vector \(\overline{x}\), and the Mahalanobis distance is the distance from the point \(x\) to the mean vector \(\overline{x}\), measured in terms of the standard deviation of the ellipsoid in the direction of \(x\).</p> <div class="l-page"> <iframe src="/assets/plotly/mahalanobis_distance_plot.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div class="caption"> Plot 1: Mahalanobis distance between a point $\color{green} X$ and its mean vector $\color{green}\overline{X}$. </div> <p>In the two dimensional case we obtain an ellipse chracterized by the position of its centroid, as well as length and orientation of the orthogonal principal axes. Direction and length of the principal axes are determined by the covariance matrix \(S\).</p> <p>But theres one problem with these parameters, which is called as eigenvalue problem.</p> <h3 id="eigenvalue-problem">Eigenvalue Problem</h3> <p>Now lets first transform the covariance matrix \(S\) into a diagonal matrix, using linear transformation (i.e preserving relevant characteristics like trace, determinant, eigenvalues, etc.): This means we are looking for a matrix \(G\) such that:</p> \[G ^ {T} S G = \Lambda\] <p>where \(\Lambda\) is a diagonal matrix of eigenvalues of \(S\) and the column vectors of \(G\) are their corresponding eigenvectors.</p> \[\Lambda = \begin{bmatrix} \lambda_1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \lambda_2 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \lambda_p \end{bmatrix}\] \[G = \begin{bmatrix} g_1 &amp; g_2 &amp; \cdots &amp; g_p \end{bmatrix}\] <p>\(\lambda_1, \lambda_2, \cdots, \lambda_p\) are the eigenvalues of \(S\) and \(g_1, g_2, \cdots, g_p\) are the corresponding eigenvectors.</p> <blockquote> <h4 id="question-1">Question</h4> <p>What is the difference between eigenvectors and eigenvalues? Eigenvectors are the vectors that do not change their direction under a given linear transformation. Eigenvalues are the scalar values that are used to transform the eigenvectors. The eigenvectors can be scaled by the eigenvalues to get the original matrix.</p> </blockquote> <p>The eigenvalues of \(S\) are the variances of the data along the principal axes, and the eigenvectors are the directions of the principal axes.</p> <p>The objective of this transformation is that the covariance matrix \(S\) is now a diagonal matrix, which means that the covariance between the variables is zero. This is called decorrelation.</p> <div class="l-page"> <iframe src="/assets/plotly/eigenvalue_plot.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div class="caption"> Plot 2: Eigenvalues of the covariance matrix $ \color{Green} S$. </div> <h3 id="total-sample-variance">Total Sample Variance</h3> <p>The matrix \(S\) provides a complete description of the variances and the covariances of the involved variables. But often we are interested in the total variance of the dataset.</p> <p>The total variance of the dataset is the sum of the variances of the variables. The total variance is the trace of the covariance matrix \(S\) given by \(tr(S)\).</p> <p>The trace of the covariance matrix, and thus the total sample variance, can be seen as a natural generalization of the variance of a single variable to multiple variables.</p> \[tr(S) = \sum_{i=1}^p \lambda_i\] <p>Now here the total sample variance is the sum of the eigenvalues of the covariance matrix \(S\).</p> <h3 id="generalized-sample-variance">Generalized Sample Variance</h3> <p>The generalized sample variance is the determinant of the covariance matrix \(S\) given by \(det(S)\) and it provides a measure of the volume of the ellipsoid.</p> \[det(S) = \prod_{i=1}^p \lambda_i\] <h3 id="correlation-matrix">Correlation Matrix</h3> <p>The correlation matrix \(R\) is a normalized version of the covariance matrix \(S\), and it is given by:</p> \[R = \frac{1}{n-1} \sum_{i=1}^n \frac{(x_i - \overline{x})(x_i - \overline{x})^T}{\sigma_i \sigma_j}\] <p>where \(\sigma_i\) and \(\sigma_j\) are the standard deviations of the variables \(x_i\) and \(x_j\) respectively.</p> <p>The correlation matrix \(R\) is a symmetric matrix with ones on the diagonal, and it is a normalized version of the covariance matrix \(S\).</p> \[R = \begin{bmatrix} 1 &amp; \rho_{12} &amp; \cdots &amp; \rho_{1p} \\ \rho_{21} &amp; 1 &amp; \cdots &amp; \rho_{2p} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \rho_{p1} &amp; \rho_{p2} &amp; \cdots &amp; 1 \end{bmatrix}\] <p>The correlation matrix can be obtained from the covariance matrix by dividing each element by the product of the standard deviations of the two variables.</p> \[\rho_{ij} = \frac{\sigma_{ij}}{\sigma_i \sigma_j}\] <h2 id="probability-theory">Probability Theory</h2> <p>The mean vector is an estimator for the expected value vector \(\mu\), the sample covariance matrix is an estimator for the covariance matrix \(\Sigma\), and the empirical correlation matrix is an estimator for the correlation matrix \(P\).</p> <p>So we have,</p> \[\overline{x} \approx \hat{\mu}\] \[S \approx \hat{\Sigma}\] \[R \approx \hat{P}\] <p>So now, the expected value for a random vector \(x\) = \([x_1, x_2, \cdots, x_p]^T\) is given by:</p> \[E(x) = \mu = \begin{bmatrix} E(x_1) \\ E(x_2) \\ \vdots \\ E(x_p) \end{bmatrix}\] <p>and the covariance matrix is given by:</p> \[Cov(x) = E[(x - \mu)(x - \mu)^T] = \Sigma = \begin{bmatrix} E(x_1 - \mu_1)^2 &amp; E((x_1 - \mu_1)(x_2 - \mu_2)) &amp; \cdots &amp; E((x_1 - \mu_1)(x_p - \mu_p)) \\ E((x_2 - \mu_2)(x_1 - \mu_1)) &amp; E(x_2 - \mu_2)^2 &amp; \cdots &amp; E((x_2 - \mu_2)(x_p - \mu_p)) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ E((x_p - \mu_p)(x_1 - \mu_1)) &amp; E((x_p - \mu_p)(x_2 - \mu_2)) &amp; \cdots &amp; E(x_p - \mu_p)^2 \end{bmatrix}\] \[= \begin{bmatrix} \sigma^2(x_1) &amp; \sigma(x_1, x_2) &amp; \cdots &amp; \sigma(x_1, x_p) \\ \sigma(x_2, x_1) &amp; \sigma^2(x_2) &amp; \cdots &amp; \sigma(x_2, x_p) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \sigma(x_p, x_1) &amp; \sigma(x_p, x_2) &amp; \cdots &amp; \sigma^2(x_p) \end{bmatrix}\] <p>where \(\sigma^2(x_i)\) is the variance of \(x_i\) and \(\sigma(x_i, x_j)\) is the covariance between \(x_i\) and \(x_j\).</p> <p>The covariance matrix is a symmetric matrix with the variances of the variables on the diagonal and the covariances between the variables on the off-diagonal.</p> <h3 id="expected-values-and-covariances-properties">Expected values and covariances properties</h3> <ol> <li>The expected value of a constant is the constant itself. \(E(c) = c\)</li> <li>The expected value of a constant times a random variable is the constant times the expected value of the random variable. \(E(cX) = cE(X)\)</li> <li>The expected value of a sum of random variables is the sum of the expected values of the random variables. \(E(X + Y) = E(X) + E(Y)\)</li> <li>The expected value of a product of random variables is the product of the expected values of the random variables. \(E(XY) = E(X)E(Y)\)</li> <li>The covariance of a constant times a random variable is the constant times the covariance of the random variable. \(Cov(cX, Y) = cCov(X, Y)\)</li> <li>The covariance of a sum of random variables is the sum of the covariances of the random variables. \(Cov(X + Y, Z) = Cov(X, Z) + Cov(Y, Z)\)</li> <li>\(Cov(X)\) is called positive semi-definite if \(x^{T}Cov(X)x \geq 0\) for all \(x \in R^p\).</li> </ol> <h3 id="multivariate-normal-distribution">Multivariate Normal Distribution</h3> <p>The \(p\)-dimensional random vector \(x\) is said to have a multivariate normal distribution with mean vector \(\mu\) and covariance matrix \(\Sigma\) if its probability density function is given by:</p> \[f(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}exp\left(-\frac{1}{2}(x - \mu)^T\Sigma^{-1}(x - \mu)\right)\] </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2023-05-05-Stats-1.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Suraj Bhor. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>